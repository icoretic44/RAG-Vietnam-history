import os
import streamlit as st
from dotenv import load_dotenv
import requests

# LangChain Imports
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import AgentExecutor, create_react_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.tools.retriever import create_retriever_tool
from langchain import hub
from langchain.tools.render import render_text_description_and_args 

st.set_page_config(page_title="H·ªèi ƒê√°p L·ªãch S·ª≠ VN", layout="wide") #st config should be the first place

load_dotenv()
VECTORSTORE_PATH = "faiss_index"
EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"
DEVICE_SETTING_EMBEDDINGS = 'cpu'
USE_OLLAMA = False #False to use Gemini
OLLAMA_MODEL = "gemma:2b"
GEMINI_MODEL = "gemini-2.0-flash"
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not TAVILY_API_KEY:
    st.error("Kh√≥a API Tavily ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t...")
    st.stop()
if not USE_OLLAMA and not GOOGLE_API_KEY:
    st.error("Kh√≥a API Google ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t...")
    st.stop()

# --- Load Resources 
@st.cache_resource
def load_embeddings_app(model_name):
    print(f"App: ƒêang t·∫£i m√¥ h√¨nh nh√∫ng: {model_name}")
    try:
        embeddings_instance = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': DEVICE_SETTING_EMBEDDINGS}
        )
        print(f"App: M√¥ h√¨nh nh√∫ng ƒë√£ ƒë∆∞·ª£c t·∫£i tr√™n {DEVICE_SETTING_EMBEDDINGS.upper()}.")
        return embeddings_instance
    except Exception as e:
        st.error(f"App: L·ªói khi t·∫£i m√¥ h√¨nh nh√∫ng '{model_name}': {e}")
        st.stop()

@st.cache_resource
def load_vectorstore_app(vectorstore_path, _embeddings_instance):
    if os.path.exists(vectorstore_path):
        print(f"App: ƒêang t·∫£i vector store t·ª´: {vectorstore_path}")
        try:
            vectorstore = FAISS.load_local(
                vectorstore_path,
                _embeddings_instance,
                allow_dangerous_deserialization=True
            )
            print("App: Vector store ƒë√£ ƒë∆∞·ª£c t·∫£i.")
            return vectorstore
        except Exception as e:
            st.error(f"App: L·ªói khi t·∫£i vector store: {e}")
            return None
    else:
        st.error(f"App: Vector store kh√¥ng t√¨m th·∫•y t·∫°i '{vectorstore_path}'. Vui l√≤ng ch·∫°y ingest_data.py tr∆∞·ªõc.")
        return None

@st.cache_resource
def get_llm_app():
    if USE_OLLAMA:
        try:
            llm_instance = ChatOllama(model=OLLAMA_MODEL, temperature=0.0)
            llm_instance.invoke("Xin ch√†o")
            print(f"App: M√¥ h√¨nh Ollama '{OLLAMA_MODEL}' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o.")
            return llm_instance
        except Exception as e:
            st.error(f"App: Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi m√¥ h√¨nh Ollama '{OLLAMA_MODEL}'. L·ªói: {e}")
            st.stop()
    else:
        try:
            llm_instance = ChatGoogleGenerativeAI(
                model=GEMINI_MODEL,
                google_api_key=GOOGLE_API_KEY,
                temperature=0.0
            )
            llm_instance.invoke("Xin ch√†o")
            print(f"App: M√¥ h√¨nh Gemini '{GEMINI_MODEL}' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o.")
            return llm_instance
        except Exception as e:
            st.error(f"App: Kh√¥ng th·ªÉ kh·ªüi t·∫°o m√¥ h√¨nh Gemini '{GEMINI_MODEL}'. L·ªói: {e}")
            st.stop()

embeddings_app = load_embeddings_app(EMBEDDING_MODEL_NAME)
vectorstore_app = load_vectorstore_app(VECTORSTORE_PATH, embeddings_app)
llm_app = get_llm_app()

# --- Streamlit UI ---

st.title("üí¨ H·ªèi ƒê√°p L·ªãch S·ª≠ Vi·ªát Nam (RAG + Tavily Search)")
st.caption("D·ª±a tr√™n t√†i li·ªáu PDF v√† t√¨m ki·∫øm web v·ªõi Tavily")

if vectorstore_app and llm_app:
    retriever = vectorstore_app.as_retriever(search_kwargs={'k': 3})

    retriever_tool = create_retriever_tool(
        retriever,
        "tim_kiem_tai_lieu_lich_su_viet_nam_pdf",
        ("C√¥ng c·ª• n√†y t√¨m ki·∫øm trong m·ªôt c∆° s·ªü d·ªØ li·ªáu PDF c·ª•c b·ªô ch·ª©a th√¥ng tin chi ti·∫øt v·ªÅ l·ªãch s·ª≠ Vi·ªát Nam, "
         "bao g·ªìm c√°c s·ª± ki·ªán, tr·∫≠n ƒë√°nh (v√≠ d·ª•: ƒë·ªìi Him Lam, ·∫§p B·∫Øc), nh√¢n v·∫≠t v√† di·ªÖn bi·∫øn li√™n quan ƒë·∫øn "
         "Chi·∫øn d·ªãch ƒêi·ªán Bi√™n Ph·ªß v√† Chi·∫øn d·ªãch H·ªì Ch√≠ Minh. "
         "H√£y S·ª¨ D·ª§NG C√îNG C·ª§ N√ÄY ƒê·∫¶U TI√äN V√Ä ∆ØU TI√äN NH·∫§T cho c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn c√°c ch·ªß ƒë·ªÅ n√†y. "
         "Ch·ªâ d√πng c√¥ng c·ª• kh√°c n·∫øu c√¥ng c·ª• n√†y kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë·∫ßy ƒë·ªß ho·∫∑c tr·∫£ v·ªÅ 'kh√¥ng c√≥ th√¥ng tin'."),
    )
    tavily_tool = TavilySearchResults(
        max_results=2,
        api_key=TAVILY_API_KEY,
        name="tim_kiem_web_tavily", # Tool name for the agent
        description="M·ªôt c√¥ng c·ª• t√¨m ki·∫øm tr√™n web. S·ª≠ d·ª•ng c√¥ng c·ª• n√†y ƒë·ªÉ t√¨m th√¥ng tin c·∫≠p nh·∫≠t ho·∫∑c c√°c ch·ªß ƒë·ªÅ kh√¥ng c√≥ trong t√†i li·ªáu l·ªãch s·ª≠ PDF, ho·∫∑c khi c√¥ng c·ª• t√¨m ki·∫øm PDF kh√¥ng cung c·∫•p ƒë·ªß th√¥ng tin."
    )
    tools = [retriever_tool, tavily_tool]

    # 4. Get the Agent Prompt Template from LangChain Hub
    try:
        prompt = hub.pull("hwchase17/react")
        if isinstance(prompt, ChatPromptTemplate):
            pass 
        elif isinstance(prompt, PromptTemplate) and isinstance(prompt.template, str):
             if "\nH√£y lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát." not in prompt.template:
                prompt.template = prompt.template + "\nH√£y lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát. ∆Øu ti√™n s·ª≠ d·ª•ng 'tim_kiem_tai_lieu_lich_su_viet_nam_pdf' tr∆∞·ªõc cho c√°c c√¢u h·ªèi l·ªãch s·ª≠ Vi·ªát Nam."
        print("Prompt from LangChain Hub loaded.")

    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ t·∫£i prompt t·ª´ LangChain Hub: {e}. S·ª≠ d·ª•ng prompt d·ª± ph√≤ng.")
        # Fallback prompt remains the same
        prompt = ChatPromptTemplate.from_messages([
            ("system", "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch t·ªët nh·∫•t c√≥ th·ªÉ. B·∫°n c√≥ quy·ªÅn truy c·∫≠p v√†o c√°c c√¥ng c·ª• sau:\n{tools}\nS·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng sau:\nC√¢u h·ªèi: c√¢u h·ªèi ƒë·∫ßu v√†o b·∫°n ph·∫£i tr·∫£ l·ªùi\nSuy nghƒ©: b·∫°n n√™n lu√¥n suy nghƒ© v·ªÅ nh·ªØng vi·ªác c·∫ßn l√†m\nH√†nh ƒë·ªông: h√†nh ƒë·ªông c·∫ßn th·ª±c hi·ªán, ph·∫£i l√† m·ªôt trong [{tool_names}]\nƒê·∫ßu v√†o h√†nh ƒë·ªông: ƒë·∫ßu v√†o cho h√†nh ƒë·ªông\nQuan s√°t: k·∫øt qu·∫£ c·ªßa h√†nh ƒë·ªông\n... (Suy nghƒ©/H√†nh ƒë·ªông/ƒê·∫ßu v√†o h√†nh ƒë·ªông/Quan s√°t n√†y c√≥ th·ªÉ l·∫∑p l·∫°i N l·∫ßn)\nSuy nghƒ©: B√¢y gi·ªù t√¥i ƒë√£ bi·∫øt c√¢u tr·∫£ l·ªùi cu·ªëi c√πng\nC√¢u tr·∫£ l·ªùi cu·ªëi c√πng: c√¢u tr·∫£ l·ªùi cu·ªëi c√πng cho c√¢u h·ªèi ƒë·∫ßu v√†o ban ƒë·∫ßu c·ªßa ng∆∞·ªùi d√πng. Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"), # Correct for ReAct
        ])


    if "agent_executor" not in st.session_state:
        try:
            agent = create_react_agent(llm_app, tools, prompt)
            st.session_state.agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                verbose=True,
                handle_parsing_errors=True,
                max_iterations=10
            )
            print("Agent Executor ƒë∆∞·ª£c t·∫°o.")
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o Agent Executor: {e}")
            st.stop()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_query := st.chat_input("C√¢u h·ªèi c·ªßa b·∫°n:"):
        st.session_state.messages.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            thinking_process = "" # To capture agent's thoughts
            with st.spinner("Agent ƒëang x·ª≠ l√Ω..."):
                try:
                    result = st.session_state.agent_executor.invoke(
                        {"input": user_query}
                    )
                    full_response = result.get("output", "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t√¨m th·∫•y c√¢u tr·∫£ l·ªùi.")

                except Exception as e:
                    full_response = f"L·ªói agent: {e}\n\nSuy nghƒ© c·ªßa Agent (n·∫øu c√≥):\n{thinking_process}"
                    st.error(full_response)

            message_placeholder.markdown(full_response)
        st.session_state.messages.append({"role": "assistant", "content": full_response})
else:
    st.error("Kh√¥ng th·ªÉ t·∫£i Vector Store ho·∫∑c LLM. H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y `ingest_data.py` v√† c·∫•u h√¨nh ƒë√∫ng.")
